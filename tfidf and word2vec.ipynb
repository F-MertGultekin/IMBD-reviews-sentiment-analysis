{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"4ca6e08418634a79824aa6c9eefbd99e","source_hash":"75d9f173","execution_start":1679066045587,"execution_millis":1808,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# IMPORTS\nfrom nltk.corpus import stopwords\nimport nltk\nimport pandas as pd\nimport re\nfrom bs4 import BeautifulSoup\nimport numpy as np\nimport gc\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix\n\n","block_group":"4ca6e08418634a79824aa6c9eefbd99e","execution_count":2,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"3c9df118d4d34c07b01ceb666b8bd208","source_hash":"770ae62e","execution_start":1679066048539,"execution_millis":771,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"df = pd.read_csv('IMDB Dataset.csv')\ndf.head(5)\n","block_group":"3c9df118d4d34c07b01ceb666b8bd208","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":2,"row_count":5,"columns":[{"name":"review","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.","count":1},{"name":"A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.","count":1},{"name":"3 others","count":3}]}},{"name":"sentiment","dtype":"object","stats":{"unique_count":2,"nan_count":0,"categories":[{"name":"positive","count":4},{"name":"negative","count":1}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"review":"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fa…","sentiment":"positive","_deepnote_index_column":"0"},{"review":"A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.","sentiment":"positive","_deepnote_index_column":"1"},{"review":"I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.","sentiment":"positive","_deepnote_index_column":"2"},{"review":"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.","sentiment":"negative","_deepnote_index_column":"3"},{"review":"Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with …","sentiment":"positive","_deepnote_index_column":"4"}]},"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/70c8e97b-85be-4d86-9082-e04b0f2d622f"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"1c0bade04eb94a86bb1d21788a88325c","source_hash":"14f60b8f","execution_start":1679000585560,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"df.shape","block_group":"1c0bade04eb94a86bb1d21788a88325c","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"(50000, 2)"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/fb2a5d1f-9192-495e-b156-6e6f3485a961"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"3aee25d2adf7409e86fb038ea7684c2f","source_hash":"5da5fb21","execution_start":1678924949546,"execution_millis":4976944,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"source":"# #several data analysis first?\n# import plotly.express as px\n# fig = px.histogram(df,x='sentiment')\n# fig.show()\n# #positive case equals to negative case\n\n# print(len(df['review'].unique()))\n# #unique review:49582\n\n# df = df.drop_duplicates()\n# print(len(df))\n# #we drop the duplicate and the remain number of the whole dataset is 49582.\n\n# #some pre-process\n# from sklearn.model_selection import train_test_split\n# X_train, X_test, y_train, y_test = train_test_split(df['review'],df['sentiment'],test_size = 0.2)\n\n# print(y_train.value_counts())\n# print(y_test.value_counts())","block_group":"3aee25d2adf7409e86fb038ea7684c2f","execution_count":0,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"0586baa804634c25a7955925d61e7756","source_hash":"74b9d010","execution_start":1678924963795,"execution_millis":4962698,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"source":"# #Define function for removing special characters\n# def RemoveNoisyCharacters(text):\n#     soup = BeautifulSoup(text, \"html.parser\") #Remove HTML tags like <br>...\n#     text = soup.get_text()\n#     pattern=r'[^a-zA-z0-9\\s]'                #Remove special characters\n#     text = re.sub(pattern,'',text)\n#     text = re.sub('\\[[^]]*\\]', '', text)     #Remove square brackets\n#     return text\n# #Apply function on review column\n# df['review']=df['review'].apply(RemoveNoisyCharacters)","block_group":"0586baa804634c25a7955925d61e7756","execution_count":0,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"86050ae57b61431da37e95282f66b3e0","source_hash":"e1a82928","execution_start":1678925055349,"execution_millis":4871149,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"source":"# #Remove Stop Words\n# def removeStopWords(wordList):\n#     newWordList = []\n#     for word in wordList:\n#         # Remove Stop Words\n#         word = word.lower()\n#         if word.lower() in stopwords:\n#             continue\n#         else:\n#             newWordList.append(word)\n#     return newWordList\n# df['review']=df['review'].apply(RemoveNoisyCharacters)\n","block_group":"86050ae57b61431da37e95282f66b3e0","execution_count":0,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"38aadbb9113c4988a4c1b9e4c13d6740","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"","block_group":"38aadbb9113c4988a4c1b9e4c13d6740"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"25859a1497ab4290ae359865fa3afb30","source_hash":"dbbe7954","execution_start":1679066054349,"execution_millis":30662,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#### Now Sun steps into the battlefield.\nnltk.download('stopwords')\nstops = set(stopwords.words('english'))\npreprocess_df = pd.DataFrame(columns=['review', 'sentiment'], index=[i for i in range(len(df))])\nfor i in range(len(df)):\n    sentence = df.loc[i,'review']\n    text = BeautifulSoup(sentence, \"html.parser\").get_text()\n    pattern=r'[^a-zA-z0-9\\s]'                #Remove special characters\n    text = re.sub(pattern,'',text)\n    text = re.sub('\\[[^]]*\\]', '', text)     #Remove square brackets\n    wordList = text.split()\n    newWordList = []\n    for word in wordList:\n        # Remove Stop Words\n        word = word.lower()\n        if word.lower() in stops:\n            continue\n        else:\n            newWordList.append(word)\n    text = ' '.join(newWordList)\n    preprocess_df.loc[i,'review'] = text\n    if df.loc[i,'sentiment'] == 'positive':\n        preprocess_df.loc[i,'sentiment'] = 1\n        df.loc[i,'sentiment'] = 1\n    else:\n        preprocess_df.loc[i,'sentiment'] = -1\n        df.loc[i,'sentiment'] = -1\nprint(preprocess_df.head(5))\npreprocess_df = preprocess_df.drop_duplicates()\npreprocess_df = preprocess_df.reset_index(drop = True)\n#we drop the duplicate and the remain number of the whole dataset is 49582.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(preprocess_df['review'],preprocess_df['sentiment'],test_size = 0.2)\n\nprint(y_train.value_counts())\nprint(y_test.value_counts())","block_group":"25859a1497ab4290ae359865fa3afb30","execution_count":4,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n/shared-libs/python3.9/py-core/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  warnings.warn(\n                                              review sentiment\n0  one reviewers mentioned watching 1 oz episode ...         1\n1  wonderful little production filming technique ...         1\n2  thought wonderful way spend time hot summer we...         1\n3  basically theres family little boy jake thinks...        -1\n4  petter matteis love time money visually stunni...         1\n 1    19835\n-1    19828\nName: sentiment, dtype: int64\n 1    5048\n-1    4868\nName: sentiment, dtype: int64\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/2138ece4-9006-4bfa-a185-b6a2729a0344"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"4dc2eb40f53a4e9d9564361b2554fb42","source_hash":"dc512fce","execution_start":1678932446307,"execution_millis":1856,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"import math\nsentence_idf = dict()\nfor i in range(len(X_train)):\n    wordlist = X_train.iloc[i].split()\n    wordset = set(wordlist)\n    for word in wordset:\n        if word in sentence_idf:\n            sentence_idf[word] += 1\n        else:\n            sentence_idf[word] = 1\nfor word, value in sentence_idf.items():\n    sentence_idf[word] = math.log(len(X_train)/value)\n\nprint(sentence_idf[\"love\"])\nprint(sentence_idf[\"worst\"])","block_group":"4dc2eb40f53a4e9d9564361b2554fb42","execution_count":null,"outputs":[{"name":"stdout","text":"1.7390905236458678\n2.431950718983582\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/3d3d8a74-5488-4f06-9426-098776e4d695"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"7610a52ed53c432fa5dfcd10cd4df0ac","source_hash":"32142491","execution_start":1678934269337,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"def get_tfidf(review, sentenceidf = sentence_idf):\n    wordlist = review.split()\n    length = len(wordlist)\n    setwordlist = set(wordlist)\n    sentence_tf = dict.fromkeys(setwordlist, 0)\n    for word in wordlist:\n        sentence_tf[word] += 1/length\n    sentence_tfidf = dict()\n    for word, value in sentence_tf.items():\n        if word in sentence_idf:\n            sentence_tfidf[word] = value * sentence_idf[word]\n    return sentence_tfidf\n\ndef predition_through_tfidf(review,X_train = X_train,y_train = y_train, sentenceidf = sentence_idf):\n    sentence_tfidf = get_tfidf(review)\n    vote = 0\n    for i in range(len(X_train)):\n        text = X_train.iloc[i]\n        X_train_tfidf = get_tfidf(text, sentenceidf)\n        for word, value in sentence_tfidf.items():\n            if word in X_train_tfidf:\n                vote += sentence_tfidf[word]*X_train_tfidf[word]*y_train.iloc[i]\n    return vote\n\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\ny_pred_real_num = []\ny_pred = []\nfor i in tqdm(range(100)): \n#3.5s for one prediction, we can only predict first 100 to cut down the time cost\n    num = predition_through_tfidf(X_test.iloc[i])\n    y_pred_real_num.append(num)\n    if num > 0:\n        y_pred.append(1)\n    else:\n        y_pred.append(-1)","block_group":"7610a52ed53c432fa5dfcd10cd4df0ac","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"4e60d6c7fd534615ae460175fb87c6b8","source_hash":"a8b70e51","execution_start":1678932823117,"execution_millis":7,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"y_true = []\nfor i in range(100):\n    y_true.append(y_test.iloc[i])\nprint(classification_report(y_true,y_pred))","block_group":"4e60d6c7fd534615ae460175fb87c6b8","execution_count":null,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n          -1       0.82      0.78      0.80        51\n           1       0.78      0.82      0.80        49\n\n    accuracy                           0.80       100\n   macro avg       0.80      0.80      0.80       100\nweighted avg       0.80      0.80      0.80       100\n\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/b479bfad-4852-42a1-a5ba-8f0bb36d9eeb"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"6afef94b87b1439397917b3d93e4fcd6","source_hash":"641c6d2a","execution_start":1678934721342,"execution_millis":1229639,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#what if we directly use the method without preprocessing?\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df['review'],df['sentiment'],test_size = 0.2)\nimport math\nsentence_idf = dict()\nfor i in range(len(X_train)):\n    wordlist = X_train.iloc[i].split()\n    wordset = set(wordlist)\n    for word in wordset:\n        if word in sentence_idf:\n            sentence_idf[word] += 1\n        else:\n            sentence_idf[word] = 1\nfor word, value in sentence_idf.items():\n    sentence_idf[word] = math.log(len(X_train)/value)\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\ny_pred_real_num = []\ny_pred = []\nfor i in tqdm(range(100)): \n#3.5s for one prediction, we can only predict first 100 to cut down the time cost\n    num = predition_through_tfidf(X_test.iloc[i],X_train,y_train,sentence_idf)\n    y_pred_real_num.append(num)\n    if num > 0:\n        y_pred.append(1)\n    else:\n        y_pred.append(-1)\ny_true = []\nfor i in range(100):\n    y_true.append(y_test.iloc[i])\n\nprint(classification_report(y_true,y_pred))\n#We can see that without preprocess, the accuracy actually improves which implies that \"stopwords\" \n#and other things that we think it is unimportant actually can contribute positively to the performance\n#However, without preprocess the model requires 12s instead of 3s for 1 prediction so it is still wise to preprocess ","block_group":"6afef94b87b1439397917b3d93e4fcd6","execution_count":null,"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [20:24<00:00, 12.24s/it]              precision    recall  f1-score   support\n\n          -1       0.86      0.93      0.89        55\n           1       0.90      0.82      0.86        45\n\n    accuracy                           0.88       100\n   macro avg       0.88      0.87      0.88       100\nweighted avg       0.88      0.88      0.88       100\n\n\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/2e61914e-833b-4d8f-a1a2-5232ca64b4e0"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"30b9687149d84e4f8302736a20eb1b94","source_hash":"98fafdab","execution_start":1679064455663,"execution_millis":109446,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#now I start to do the word2vec\n!pip install gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\ncf = df['review'].apply(simple_preprocess)\nmodel = Word2Vec()\nmodel.build_vocab(cf)\nmodel.train(cf,total_examples=model.corpus_count, epochs=model.epochs)\nmodel.save('./word2vec.model')\n","block_group":"30b9687149d84e4f8302736a20eb1b94","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting gensim\n  Downloading gensim-4.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy>=1.7.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gensim) (1.9.3)\nRequirement already satisfied: numpy>=1.18.5 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gensim) (1.23.4)\nRequirement already satisfied: smart-open>=1.8.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gensim) (5.2.1)\nInstalling collected packages: gensim\nSuccessfully installed gensim-4.3.1\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/1634ed2b-408b-4745-983a-10336c917e3c"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"f43b55c263f840d5832bd79ef9dea4aa","source_hash":"157c792a","execution_start":1679064860639,"execution_millis":790,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"cf","block_group":"f43b55c263f840d5832bd79ef9dea4aa","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"0        [one, of, the, other, reviewers, has, mentione...\n1        [wonderful, little, production, br, br, the, f...\n2        [thought, this, was, wonderful, way, to, spend...\n3        [basically, there, family, where, little, boy,...\n4        [petter, mattei, love, in, the, time, of, mone...\n                               ...                        \n49995    [thought, this, movie, did, down, right, good,...\n49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n49997    [am, catholic, taught, in, parochial, elementa...\n49998    [going, to, have, to, disagree, with, the, pre...\n49999    [no, one, expects, the, star, trek, movies, to...\nName: review, Length: 50000, dtype: object"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/40a10df3-b68b-46d7-b83a-ba259cdedd34"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"bf29d05cc52d4c379691ba0b73c97ce7","source_hash":"ca317d7a","execution_start":1679064801527,"execution_millis":54622,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"!pip install gensim\nfrom gensim.models import Word2Vec\nimport numpy as np\nmodel = Word2Vec.load('word2vec.model')\nvoc = model.wv\nX_train, X_test, y_train, y_test = train_test_split(preprocess_df['review'],preprocess_df['sentiment'],test_size = 0.2)\ndef average_vector(sentence, voc, vector_length = 100):\n    wordlist = sentence.split()\n    length = len(wordlist)\n    avg_vector = np.zeros((vector_length,), dtype = float)\n    count = 0\n    for i in range(length):\n        word = wordlist[i]\n        if word in voc:\n            avg_vector += voc[word]\n            count += 1\n    if count != 0:\n        avg_vector/= count\n    else:\n        avg_vector = np.zeros((vector_length,), dtype = float)\n    return avg_vector\n\nfrom sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier()\nvector_train = []\nvector_test = []\nfor i in range(len(X_train)):\n    vector_train.append(average_vector(X_train.iloc[i],voc))\nfor i in range(len(X_test)):\n    vector_test.append(average_vector(X_test.iloc[i],voc))\nprint(\"start train:\")\nforest = forest.fit(vector_train,y_train.astype(int))\nprint(\"start test:\")\ny_pred = forest.predict(vector_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test.astype(int),y_pred))","block_group":"bf29d05cc52d4c379691ba0b73c97ce7","execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gensim in /root/venv/lib/python3.9/site-packages (4.3.1)\nRequirement already satisfied: smart-open>=1.8.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gensim) (5.2.1)\nRequirement already satisfied: numpy>=1.18.5 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gensim) (1.23.4)\nRequirement already satisfied: scipy>=1.7.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gensim) (1.9.3)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mstart train:\nstart test:\n              precision    recall  f1-score   support\n\n          -1       0.83      0.80      0.82      4938\n           1       0.81      0.84      0.82      4978\n\n    accuracy                           0.82      9916\n   macro avg       0.82      0.82      0.82      9916\nweighted avg       0.82      0.82      0.82      9916\n\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/bcc62744-465f-48d0-80f7-29cb896d3232"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"6e9780d94b784e42b28a4dba66f0aa3e","source_hash":"9a7d3b8e","execution_start":1679064920674,"execution_millis":68794,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#maybe average is not a good idea what about we add the weight of the tf idf value?\nimport math\nsentence_idf = dict()\nfor i in range(len(X_train)):\n    wordlist = X_train.iloc[i].split()\n    wordset = set(wordlist)\n    for word in wordset:\n        if word in sentence_idf:\n            sentence_idf[word] += 1\n        else:\n            sentence_idf[word] = 1\nfor word, value in sentence_idf.items():\n    sentence_idf[word] = math.log(len(X_train)/value)\n\ndef get_tfidf(review, sentenceidf = sentence_idf):\n    wordlist = review.split()\n    length = len(wordlist)\n    setwordlist = set(wordlist)\n    sentence_tf = dict.fromkeys(setwordlist, 0)\n    for word in wordlist:\n        sentence_tf[word] += 1/length\n    sentence_tfidf = dict()\n    for word, value in sentence_tf.items():\n        if word in sentence_idf:\n            sentence_tfidf[word] = value * sentence_idf[word]\n    return sentence_tfidf\n\ndef weighted_average_vector(sentence, voc, vector_length = 100, sentenceidf = sentence_idf):\n    wordlist = sentence.split()\n    sentence_tfidf = get_tfidf(sentence,sentenceidf)\n    length = len(wordlist)\n    avg_vector = np.zeros((vector_length,), dtype = float)\n    count = 0\n    for i in range(length):\n        word = wordlist[i]\n        if word in voc and word in sentence_tfidf:\n            # avg_vector += voc[word]*sentenceidf[word]\n            # count += sentenceidf[word]\n            avg_vector += voc[word]*sentence_tfidf[word]\n            count += sentence_tfidf[word]\n    if count != 0:\n        avg_vector/= count\n    else:\n        avg_vector = np.zeros((vector_length,), dtype = float)\n    return avg_vector\n\n\nmodel = Word2Vec.load('word2vec.model')\nvoc = model.wv\nforest = RandomForestClassifier()\nvector_train = []\nvector_test = []\nfrom tqdm import tqdm\nfor i in tqdm(range(len(X_train))):\n    vector_train.append(weighted_average_vector(X_train.iloc[i],voc))\nfor i in tqdm(range(len(X_test))):\n    vector_test.append(weighted_average_vector(X_test.iloc[i],voc))\nprint(\"start train:\")\nforest = forest.fit(vector_train,y_train.astype(int))\nprint(\"start test:\")\ny_pred = forest.predict(vector_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test.astype(int),y_pred))\n#We can see that the tfidf weight is not working, it does not improve the result","block_group":"6e9780d94b784e42b28a4dba66f0aa3e","execution_count":null,"outputs":[{"name":"stderr","text":"100%|██████████| 39663/39663 [00:23<00:00, 1657.96it/s]\n100%|██████████| 9916/9916 [00:05<00:00, 1695.33it/s]\nstart train:\nstart test:\n              precision    recall  f1-score   support\n\n          -1       0.82      0.78      0.80      4938\n           1       0.79      0.83      0.81      4978\n\n    accuracy                           0.80      9916\n   macro avg       0.80      0.80      0.80      9916\nweighted avg       0.80      0.80      0.80      9916\n\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/08f00058-2945-410f-9a9f-c332669e6435"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"2dd8c22787cb4cccb339576d1f1e43d2","source_hash":"b63ff1d6","execution_start":1679066212844,"execution_millis":7527,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#so maybe just to take an average is not enough for the text, we need to use the information of the context.\n#As a result we may need to use some kind of RNN.\n# !pip install tensorflow\n# !pip install gensim\nfrom tensorflow import keras\nfrom keras.initializers import Constant\nfrom gensim.models import Word2Vec\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nX_train, X_test, y_train, y_test = train_test_split(preprocess_df['review'],preprocess_df['sentiment'],test_size = 0.2)\nX_train = X_train.reset_index()\nX_test = X_test.reset_index()\ny_train = y_train.reset_index()\ny_test = y_test.reset_index()\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(preprocess_df['review'])\nsequence_train = tokenizer.texts_to_sequences(X_train)\nsequence_test = tokenizer.texts_to_sequences(X_test)\nx_train = keras.preprocessing.sequence.pad_sequences(sequence_train,padding = 'post',maxlen = 100)\nx_test = keras.preprocessing.sequence.pad_sequences(sequence_test,padding = 'post',maxlen = 100)\nvocab_size = len(tokenizer.word_index) + 1\n\nfrom tqdm import tqdm\nword2vec_model = Word2Vec.load('word2vec.model')\nvoc = word2vec_model.wv\nembedding_matrix = dict()\nembedding_matrix[0] = np.zeros((100,), dtype = float)\nfor word in tokenizer.word_index:\n    idx = tokenizer.word_index[word]\n    if word in voc:\n        embedding_matrix[idx] = voc[word]\n    else:\n        embedding_matrix[idx] = np.zeros((100,), dtype = float)\n\nLSTM_model = keras.Sequential()\nLSTM_model.add(keras.layers.Embedding(input_dim=vocab_size,output_dim=100,embeddings_initializer=Constant(embedding_matrix)))\nLSTM_model.add(keras.layers.LSTM(64,return_sequences = True))\nLSTM_model.add(keras.layers.LSTM(64,return_sequences = True))\nLSTM_model.add(keras.layers.LSTM(128,return_sequences = True))\nLSTM_model.add(keras.layers.Dense(10,activation='relu'))\nLSTM_model.add(keras.layers.Dense(1,activation='sigmoid'))\nopt = keras.optimizers.Adam(lr = 0.001)\nLSTM_model.compile(optimizer = opt, loss = 'binary_crossentropy',metrics=['acc'])\nLSTM_model.fit(x_train,y_train.astype(int),epochs=20,validation_data=(X_test,y_test.astype(int)),batch_size = 64)\nloss, acc = LSTM_model.evaluate(x_test, y_test, verbose = False)\nprint(\"Testing accuracy: \",loss, acc)\n","block_group":"2dd8c22787cb4cccb339576d1f1e43d2","execution_count":1,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"d1aa6d6416bd422abf95be3ca3c530a9","source_hash":"99ba196a","execution_start":1679065462717,"execution_millis":5,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"tokenizer.word_index","block_group":"d1aa6d6416bd422abf95be3ca3c530a9","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"{'movie': 1,\n 'film': 2,\n 'one': 3,\n 'like': 4,\n 'good': 5,\n 'even': 6,\n 'would': 7,\n 'time': 8,\n 'really': 9,\n 'see': 10,\n 'story': 11,\n 'much': 12,\n 'well': 13,\n 'get': 14,\n 'great': 15,\n 'bad': 16,\n 'also': 17,\n 'people': 18,\n 'first': 19,\n 'dont': 20,\n 'movies': 21,\n 'made': 22,\n 'films': 23,\n 'make': 24,\n 'could': 25,\n 'way': 26,\n 'characters': 27,\n 'think': 28,\n 'watch': 29,\n 'many': 30,\n 'seen': 31,\n 'character': 32,\n 'two': 33,\n 'never': 34,\n 'love': 35,\n 'acting': 36,\n 'best': 37,\n 'plot': 38,\n 'little': 39,\n 'know': 40,\n 'show': 41,\n 'life': 42,\n 'ever': 43,\n 'better': 44,\n 'say': 45,\n 'still': 46,\n 'scene': 47,\n 'end': 48,\n 'man': 49,\n 'scenes': 50,\n 'something': 51,\n 'go': 52,\n 'back': 53,\n 'real': 54,\n 'im': 55,\n 'watching': 56,\n 'thing': 57,\n 'doesnt': 58,\n 'didnt': 59,\n 'actors': 60,\n 'years': 61,\n 'actually': 62,\n 'though': 63,\n 'makes': 64,\n 'funny': 65,\n 'another': 66,\n 'find': 67,\n 'nothing': 68,\n 'look': 69,\n 'going': 70,\n 'work': 71,\n 'lot': 72,\n 'new': 73,\n 'every': 74,\n 'old': 75,\n 'us': 76,\n 'part': 77,\n 'cant': 78,\n 'director': 79,\n 'want': 80,\n 'quite': 81,\n 'thats': 82,\n 'things': 83,\n 'pretty': 84,\n 'cast': 85,\n 'seems': 86,\n 'around': 87,\n 'young': 88,\n 'got': 89,\n 'take': 90,\n 'fact': 91,\n 'world': 92,\n 'enough': 93,\n 'big': 94,\n 'horror': 95,\n 'thought': 96,\n 'give': 97,\n 'may': 98,\n 'ive': 99,\n 'however': 100,\n 'without': 101,\n 'long': 102,\n 'saw': 103,\n 'isnt': 104,\n 'gets': 105,\n 'always': 106,\n 'right': 107,\n 'music': 108,\n 'almost': 109,\n 'original': 110,\n 'must': 111,\n 'come': 112,\n 'comedy': 113,\n 'times': 114,\n 'whole': 115,\n 'role': 116,\n 'least': 117,\n 'series': 118,\n 'interesting': 119,\n 'guy': 120,\n 'action': 121,\n 'point': 122,\n 'bit': 123,\n 'done': 124,\n 'theres': 125,\n 'might': 126,\n 'far': 127,\n 'minutes': 128,\n 'hes': 129,\n 'feel': 130,\n 'script': 131,\n 'anything': 132,\n 'last': 133,\n 'since': 134,\n 'probably': 135,\n 'family': 136,\n 'kind': 137,\n 'performance': 138,\n 'tv': 139,\n 'away': 140,\n 'rather': 141,\n 'yet': 142,\n 'worst': 143,\n 'fun': 144,\n 'found': 145,\n 'anyone': 146,\n 'sure': 147,\n 'played': 148,\n 'girl': 149,\n 'making': 150,\n 'believe': 151,\n 'woman': 152,\n 'trying': 153,\n 'shows': 154,\n 'especially': 155,\n 'course': 156,\n 'comes': 157,\n 'hard': 158,\n 'goes': 159,\n 'day': 160,\n 'put': 161,\n 'everything': 162,\n 'although': 163,\n 'different': 164,\n 'worth': 165,\n 'looking': 166,\n 'dvd': 167,\n 'main': 168,\n 'ending': 169,\n 'place': 170,\n 'looks': 171,\n 'wasnt': 172,\n 'watched': 173,\n 'maybe': 174,\n 'book': 175,\n 'sense': 176,\n 'reason': 177,\n 'screen': 178,\n 'three': 179,\n 'set': 180,\n 'effects': 181,\n 'plays': 182,\n 'true': 183,\n 'someone': 184,\n 'play': 185,\n 'job': 186,\n 'money': 187,\n 'actor': 188,\n 'together': 189,\n 'said': 190,\n 'takes': 191,\n 'seem': 192,\n '10': 193,\n 'american': 194,\n 'everyone': 195,\n '2': 196,\n 'instead': 197,\n 'special': 198,\n 'beautiful': 199,\n 'seeing': 200,\n 'later': 201,\n 'left': 202,\n 'audience': 203,\n 'war': 204,\n 'version': 205,\n 'excellent': 206,\n 'john': 207,\n 'idea': 208,\n 'night': 209,\n 'shot': 210,\n 'black': 211,\n 'high': 212,\n 'simply': 213,\n 'youre': 214,\n 'fan': 215,\n 'completely': 216,\n 'used': 217,\n 'house': 218,\n 'nice': 219,\n 'read': 220,\n 'wife': 221,\n 'else': 222,\n 'poor': 223,\n 'death': 224,\n 'kids': 225,\n 'short': 226,\n 'friends': 227,\n 'help': 228,\n 'along': 229,\n 'men': 230,\n 'second': 231,\n 'mind': 232,\n 'star': 233,\n 'home': 234,\n 'need': 235,\n 'use': 236,\n 'less': 237,\n 'given': 238,\n 'try': 239,\n 'enjoy': 240,\n 'half': 241,\n 'year': 242,\n 'boring': 243,\n 'truly': 244,\n 'classic': 245,\n 'either': 246,\n 'rest': 247,\n 'performances': 248,\n 'recommend': 249,\n 'couple': 250,\n 'production': 251,\n 'wrong': 252,\n 'dead': 253,\n 'line': 254,\n 'father': 255,\n 'hollywood': 256,\n 'start': 257,\n 'tell': 258,\n 'stupid': 259,\n 'next': 260,\n 'came': 261,\n 'women': 262,\n 'getting': 263,\n 'understand': 264,\n 'remember': 265,\n 'full': 266,\n 'let': 267,\n 'school': 268,\n 'keep': 269,\n 'camera': 270,\n 'others': 271,\n 'terrible': 272,\n 'moments': 273,\n 'awful': 274,\n 'mean': 275,\n 'wonderful': 276,\n 'sex': 277,\n 'perhaps': 278,\n 'playing': 279,\n 'video': 280,\n 'name': 281,\n 'gives': 282,\n 'human': 283,\n 'often': 284,\n 'budget': 285,\n 'small': 286,\n 'definitely': 287,\n 'perfect': 288,\n 'couldnt': 289,\n 'episode': 290,\n 'person': 291,\n 'early': 292,\n 'top': 293,\n 'went': 294,\n 'dialogue': 295,\n 'absolutely': 296,\n 'guys': 297,\n 'piece': 298,\n 'lines': 299,\n 'face': 300,\n 'become': 301,\n 'certainly': 302,\n 'liked': 303,\n 'stars': 304,\n 'case': 305,\n 'sort': 306,\n 'head': 307,\n 'loved': 308,\n 'hope': 309,\n 'entire': 310,\n 'supposed': 311,\n 'felt': 312,\n 'lost': 313,\n 'several': 314,\n 'style': 315,\n 'mother': 316,\n 'live': 317,\n 'entertaining': 318,\n 'written': 319,\n 'picture': 320,\n 'finally': 321,\n 'worse': 322,\n 'laugh': 323,\n 'title': 324,\n 'waste': 325,\n 'problem': 326,\n 'friend': 327,\n 'boy': 328,\n 'shes': 329,\n 'sound': 330,\n 'beginning': 331,\n 'seemed': 332,\n 'totally': 333,\n 'wanted': 334,\n 'based': 335,\n 'yes': 336,\n 'mr': 337,\n 'cinema': 338,\n 'dark': 339,\n 'care': 340,\n 'becomes': 341,\n 'fans': 342,\n 'humor': 343,\n 'already': 344,\n 'lead': 345,\n 'wont': 346,\n 'example': 347,\n 'final': 348,\n 'guess': 349,\n '3': 350,\n 'youll': 351,\n 'called': 352,\n 'evil': 353,\n 'direction': 354,\n 'children': 355,\n 'lives': 356,\n 'able': 357,\n 'id': 358,\n 'game': 359,\n 'turn': 360,\n 'throughout': 361,\n 'low': 362,\n 'drama': 363,\n 'wants': 364,\n 'white': 365,\n 'oh': 366,\n 'girls': 367,\n 'days': 368,\n 'fine': 369,\n 'despite': 370,\n 'quality': 371,\n 'amazing': 372,\n 'horrible': 373,\n 'theyre': 374,\n 'history': 375,\n 'writing': 376,\n 'kill': 377,\n 'works': 378,\n 'tries': 379,\n 'turns': 380,\n 'enjoyed': 381,\n 'gave': 382,\n 'act': 383,\n 'unfortunately': 384,\n 'killer': 385,\n 'son': 386,\n 'michael': 387,\n 'side': 388,\n 'past': 389,\n 'favorite': 390,\n 'parts': 391,\n 'flick': 392,\n 'behind': 393,\n 'matter': 394,\n 'expect': 395,\n '1': 396,\n 'car': 397,\n 'run': 398,\n 'town': 399,\n 'brilliant': 400,\n 'ones': 401,\n 'starts': 402,\n 'stuff': 403,\n 'eyes': 404,\n 'obviously': 405,\n 'sometimes': 406,\n 'says': 407,\n 'viewer': 408,\n 'killed': 409,\n 'thinking': 410,\n 'took': 411,\n 'soon': 412,\n 'group': 413,\n 'overall': 414,\n 'directed': 415,\n 'heart': 416,\n 'actress': 417,\n 'decent': 418,\n 'late': 419,\n 'city': 420,\n 'art': 421,\n 'heard': 422,\n 'genre': 423,\n 'happens': 424,\n 'feeling': 425,\n 'kid': 426,\n 'highly': 427,\n 'hell': 428,\n 'fight': 429,\n 'child': 430,\n 'except': 431,\n 'extremely': 432,\n 'leave': 433,\n 'cannot': 434,\n 'ill': 435,\n 'wouldnt': 436,\n 'close': 437,\n 'told': 438,\n 'blood': 439,\n 'lack': 440,\n 'experience': 441,\n 'police': 442,\n 'hour': 443,\n 'moment': 444,\n 'coming': 445,\n 'etc': 446,\n 'wonder': 447,\n 'looked': 448,\n 'stories': 449,\n 'strong': 450,\n 'involved': 451,\n 'daughter': 452,\n 'chance': 453,\n 'score': 454,\n 'particularly': 455,\n 'type': 456,\n 'hand': 457,\n 'including': 458,\n 'attempt': 459,\n 'living': 460,\n 'save': 461,\n 'complete': 462,\n 'serious': 463,\n 'simple': 464,\n 'shown': 465,\n 'taken': 466,\n 'god': 467,\n 'roles': 468,\n 'obvious': 469,\n 'happened': 470,\n 'happen': 471,\n 'hilarious': 472,\n 'stop': 473,\n 'violence': 474,\n 'cool': 475,\n 'across': 476,\n 'james': 477,\n 'known': 478,\n 'murder': 479,\n 'voice': 480,\n 'exactly': 481,\n 'song': 482,\n 'opening': 483,\n 'ago': 484,\n 'number': 485,\n 'robert': 486,\n 'usually': 487,\n 'released': 488,\n 'slow': 489,\n 'saying': 490,\n 'huge': 491,\n 'ok': 492,\n 'wish': 493,\n 'order': 494,\n 'david': 495,\n 'cinematography': 496,\n 'interest': 497,\n 'whose': 498,\n 'jokes': 499,\n 'hours': 500,\n 'reality': 501,\n 'english': 502,\n 'alone': 503,\n 'crap': 504,\n 'relationship': 505,\n 'sad': 506,\n 'hit': 507,\n 'lets': 508,\n 'running': 509,\n 'none': 510,\n 'shots': 511,\n 'possible': 512,\n 'talent': 513,\n 'gore': 514,\n 'cut': 515,\n 'seriously': 516,\n 'age': 517,\n 'annoying': 518,\n 'career': 519,\n 'today': 520,\n 'started': 521,\n 'call': 522,\n 'major': 523,\n 'brother': 524,\n 'female': 525,\n 'usual': 526,\n 'taking': 527,\n 'please': 528,\n 'hero': 529,\n 'ends': 530,\n 'somewhat': 531,\n 'knew': 532,\n 'important': 533,\n 'view': 534,\n 'ridiculous': 535,\n 'mostly': 536,\n 'documentary': 537,\n 'knows': 538,\n 'beyond': 539,\n 'change': 540,\n 'body': 541,\n 'finds': 542,\n 'turned': 543,\n 'power': 544,\n 'opinion': 545,\n 'words': 546,\n 'silly': 547,\n 'strange': 548,\n 'word': 549,\n 'due': 550,\n 'novel': 551,\n 'upon': 552,\n '5': 553,\n 'scary': 554,\n 'talking': 555,\n 'apparently': 556,\n 'directors': 557,\n 'attention': 558,\n 'local': 559,\n 'anyway': 560,\n 'husband': 561,\n 'happy': 562,\n 'clearly': 563,\n 'musical': 564,\n 'episodes': 565,\n 'cheap': 566,\n 'four': 567,\n 'single': 568,\n 'room': 569,\n 'level': 570,\n 'problems': 571,\n 'arent': 572,\n 'basically': 573,\n 'country': 574,\n '4': 575,\n 'disappointed': 576,\n 'tells': 577,\n 'sequence': 578,\n 'british': 579,\n 'events': 580,\n 'television': 581,\n 'modern': 582,\n 'miss': 583,\n 'talk': 584,\n 'light': 585,\n 'songs': 586,\n 'whats': 587,\n 'appears': 588,\n 'whether': 589,\n 'french': 590,\n 'giving': 591,\n 'easily': 592,\n 'sets': 593,\n 'havent': 594,\n 'supporting': 595,\n 'bring': 596,\n 'predictable': 597,\n 'review': 598,\n 'add': 599,\n 'rating': 600,\n 'mention': 601,\n 'dialog': 602,\n 'falls': 603,\n 'lots': 604,\n 'similar': 605,\n 'soundtrack': 606,\n 'jack': 607,\n 'romantic': 608,\n 'needs': 609,\n 'team': 610,\n 'bunch': 611,\n 'george': 612,\n 'viewers': 613,\n 'enjoyable': 614,\n 'future': 615,\n 'tried': 616,\n 'ten': 617,\n 'hate': 618,\n 'certain': 619,\n 'message': 620,\n 'five': 621,\n 'showing': 622,\n 'moving': 623,\n 'surprised': 624,\n 'within': 625,\n 'entertainment': 626,\n 'comic': 627,\n 'clear': 628,\n 'earth': 629,\n 'parents': 630,\n 'storyline': 631,\n 'middle': 632,\n 'space': 633,\n 'among': 634,\n 'filmed': 635,\n 'named': 636,\n 'fall': 637,\n 'theme': 638,\n 'dull': 639,\n 'easy': 640,\n 'theater': 641,\n 'king': 642,\n 'feels': 643,\n 'ways': 644,\n 'typical': 645,\n 'greatest': 646,\n 'kept': 647,\n 'release': 648,\n 'thriller': 649,\n 'near': 650,\n 'using': 651,\n 'comments': 652,\n 'stay': 653,\n 'sequel': 654,\n 'buy': 655,\n 'doubt': 656,\n 'gone': 657,\n 'working': 658,\n 'deal': 659,\n 'effort': 660,\n 'actual': 661,\n 'writer': 662,\n 'nearly': 663,\n 'monster': 664,\n 'elements': 665,\n 'sorry': 666,\n 'means': 667,\n 'tale': 668,\n 'fantastic': 669,\n 'brought': 670,\n 'famous': 671,\n 'suspense': 672,\n '80s': 673,\n 'die': 674,\n 'subject': 675,\n 'editing': 676,\n 'straight': 677,\n 'boys': 678,\n 'rock': 679,\n 'realistic': 680,\n 'points': 681,\n 'imagine': 682,\n 'general': 683,\n 'feature': 684,\n 'sister': 685,\n 'leads': 686,\n 'lady': 687,\n 'move': 688,\n 'viewing': 689,\n 'class': 690,\n 'peter': 691,\n 'dr': 692,\n 'mystery': 693,\n 'richard': 694,\n 'youve': 695,\n 'form': 696,\n 'begins': 697,\n 'reviews': 698,\n 'hear': 699,\n 'material': 700,\n 'somehow': 701,\n 'check': 702,\n 'dog': 703,\n 'learn': 704,\n 'forget': 705,\n 'whos': 706,\n 'animation': 707,\n 'particular': 708,\n 'figure': 709,\n 'premise': 710,\n 'weak': 711,\n 'killing': 712,\n 'sit': 713,\n 'rent': 714,\n 'period': 715,\n 'believable': 716,\n 'expected': 717,\n 'surprise': 718,\n 'sequences': 719,\n 'decided': 720,\n 'atmosphere': 721,\n 'paul': 722,\n 'eventually': 723,\n 'crime': 724,\n 'deep': 725,\n 'wait': 726,\n 'difficult': 727,\n 'avoid': 728,\n 'fast': 729,\n 'red': 730,\n 'leaves': 731,\n 'tom': 732,\n 'eye': 733,\n 'became': 734,\n 'truth': 735,\n 'okay': 736,\n 'york': 737,\n 'possibly': 738,\n 'indeed': 739,\n 'emotional': 740,\n 'situation': 741,\n 'follow': 742,\n 'average': 743,\n 'poorly': 744,\n 'forced': 745,\n 'lame': 746,\n 'dance': 747,\n 'meet': 748,\n 'needed': 749,\n 'stand': 750,\n 'footage': 751,\n 'begin': 752,\n 'shame': 753,\n 'whatever': 754,\n 'scifi': 755,\n 'oscar': 756,\n 'crew': 757,\n 'open': 758,\n 'memorable': 759,\n 'minute': 760,\n 'features': 761,\n 'reading': 762,\n 'meets': 763,\n 'doctor': 764,\n 'previous': 765,\n 'filmmakers': 766,\n 'season': 767,\n 'question': 768,\n 'sexual': 769,\n 'write': 770,\n 'romance': 771,\n 'beauty': 772,\n 'interested': 773,\n 'credits': 774,\n 'writers': 775,\n 'personal': 776,\n 'gay': 777,\n 'cheesy': 778,\n 'imdb': 779,\n 'keeps': 780,\n 'third': 781,\n 'hot': 782,\n 'superb': 783,\n 'western': 784,\n 'unless': 785,\n 'nature': 786,\n 'society': 787,\n 'inside': 788,\n 'hands': 789,\n 'total': 790,\n 'otherwise': 791,\n 'perfectly': 792,\n 'towards': 793,\n 'note': 794,\n 'male': 795,\n 'incredibly': 796,\n 'sounds': 797,\n 'weird': 798,\n 'box': 799,\n 'result': 800,\n 'de': 801,\n 'screenplay': 802,\n '20': 803,\n 'effect': 804,\n 'japanese': 805,\n 'comment': 806,\n 'battle': 807,\n 'quickly': 808,\n 'masterpiece': 809,\n 'street': 810,\n 'worked': 811,\n 'realize': 812,\n 'plus': 813,\n 'laughs': 814,\n 'unique': 815,\n 'badly': 816,\n 'crazy': 817,\n 'copy': 818,\n 'older': 819,\n 'earlier': 820,\n 'various': 821,\n 'brings': 822,\n 'america': 823,\n 'appear': 824,\n 'background': 825,\n 'stage': 826,\n 'air': 827,\n 'plenty': 828,\n 'setting': 829,\n 'fairly': 830,\n 'free': 831,\n 'island': 832,\n 'b': 833,\n 'admit': 834,\n 'lee': 835,\n 'ask': 836,\n 'powerful': 837,\n 'front': 838,\n 'portrayed': 839,\n 'creepy': 840,\n 'business': 841,\n 'directing': 842,\n 'dramatic': 843,\n 'leading': 844,\n 'spent': 845,\n 'forward': 846,\n 'acted': 847,\n 'following': 848,\n 'joe': 849,\n 'rich': 850,\n 'development': 851,\n 'remake': 852,\n 'mark': 853,\n 'baby': 854,\n 'pay': 855,\n 'mess': 856,\n 'manages': 857,\n 'outside': 858,\n 'meant': 859,\n 'deserves': 860,\n 'dumb': 861,\n 'attempts': 862,\n 'political': 863,\n 'fire': 864,\n '70s': 865,\n 'fails': 866,\n 'expecting': 867,\n 'caught': 868,\n 'create': 869,\n 'water': 870,\n 'agree': 871,\n 'ideas': 872,\n 'hardly': 873,\n 'bill': 874,\n 'wasted': 875,\n 'cover': 876,\n 'fighting': 877,\n 'return': 878,\n 'reasons': 879,\n 'missing': 880,\n 'party': 881,\n 'joke': 882,\n 'apart': 883,\n 'present': 884,\n 'recently': 885,\n 'success': 886,\n 'secret': 887,\n 'cop': 888,\n 'twist': 889,\n 'la': 890,\n 'large': 891,\n 'brothers': 892,\n 'william': 893,\n 'girlfriend': 894,\n 'plain': 895,\n 'members': 896,\n 'clever': 897,\n 'ended': 898,\n 'cute': 899,\n 'talented': 900,\n 'dream': 901,\n 'telling': 902,\n 'laughing': 903,\n 'zombie': 904,\n 'escape': 905,\n 'pure': 906,\n 'german': 907,\n 'odd': 908,\n 'break': 909,\n 'disney': 910,\n 'potential': 911,\n 'waiting': 912,\n 'gun': 913,\n 'visual': 914,\n 'missed': 915,\n 'hold': 916,\n 'created': 917,\n 'pace': 918,\n 'sees': 919,\n 'married': 920,\n 'unlike': 921,\n 'match': 922,\n 'slightly': 923,\n 'casting': 924,\n 'cause': 925,\n 'incredible': 926,\n 'villain': 927,\n 'van': 928,\n 'decides': 929,\n 'italian': 930,\n 'uses': 931,\n 'nudity': 932,\n 'public': 933,\n 'familiar': 934,\n 'died': 935,\n 'considering': 936,\n 'cartoon': 937,\n 'mentioned': 938,\n 'entirely': 939,\n 'popular': 940,\n 'speak': 941,\n 'state': 942,\n 'fantasy': 943,\n 'list': 944,\n 'wrote': 945,\n 'era': 946,\n 'credit': 947,\n 'amount': 948,\n 'train': 949,\n 'appreciate': 950,\n 'produced': 951,\n 'company': 952,\n 'convincing': 953,\n 'neither': 954,\n 'rate': 955,\n 'suddenly': 956,\n 'moves': 957,\n 'follows': 958,\n 'fit': 959,\n 'compared': 960,\n 'younger': 961,\n 'tension': 962,\n 'biggest': 963,\n 'portrayal': 964,\n 'bored': 965,\n 'former': 966,\n 'flat': 967,\n 'common': 968,\n 'sadly': 969,\n 'intelligent': 970,\n 'store': 971,\n 'images': 972,\n 'dancing': 973,\n 'office': 974,\n 'audiences': 975,\n 'trouble': 976,\n 'producers': 977,\n 'science': 978,\n 'sweet': 979,\n 'value': 980,\n 'language': 981,\n 'social': 982,\n 'violent': 983,\n 'choice': 984,\n 'filled': 985,\n 'kills': 986,\n 'successful': 987,\n 'college': 988,\n 'force': 989,\n 'cold': 990,\n 'fear': 991,\n 'spend': 992,\n 'decide': 993,\n 'focus': 994,\n 'recent': 995,\n 'mad': 996,\n 'concept': 997,\n 'hair': 998,\n 'bizarre': 999,\n 'century': 1000,\n ...}"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/21331225-184a-4bcc-bdde-727d593ba559"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"2b7a8eaf2afd40d5b1c6828c66dfab7c","source_hash":"911b740e","execution_start":1678924920463,"execution_millis":5006060,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"source":"# #SKLEARN FRAMEWORK IMPLEMENTATION\n\n# #Count vectorizer for bag of words\n# cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n# #transformed train reviews\n# cv_reviews=cv.fit_transform(df['review'])\n# #transformed test reviews\n\n# #TF_IDF VECTORIZER\n# tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n# #transformed train reviews\n# tv_reviews=tv.fit_transform(df['review'])\n# print(tv_reviews)\n\n# #dataset split\n# X_train, X_test, y_train, y_test = train_test_split(df, labels, test_size=0.2, random_state=42)\n\n# #Multinomial Naive Bayes Model\n# nb_classifier = MultinomialNB()\n# nb_classifier.fit(X_train, y_train)\n\n# # Make predictions on the testing set\n# y_pred = nb_classifier.predict(X_test)\n\n# nb_classifier_report = classification_report(y_test,y_pred,target_names=['Positive','Negative'])\n# print(nb_classifier_report)\n# confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n# cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n# cm_display.plot()\n# plt.show()\n","block_group":"2b7a8eaf2afd40d5b1c6828c66dfab7c","execution_count":0,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"feb7b1caac9549489d569457f77a24cb","source_hash":"bd2edfb3","execution_start":1678719975311,"execution_millis":209951212,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"source":"# #TF-IDF Part Implementation Without SkLearn\n# def calculateTermFrequency(wordList,numberOfDocument,documentIndex):\n#     wordList = removeStopWords(wordList)\n#     numberOfWords = len(wordList)\n#     termFrequency = {}\n#     for word in wordList:\n#         if word not in termFrequency:\n#             termFrequency[word] = 1/ numberOfWords\n#         else:\n#             termFrequency[word] = (1/numberOfWords) + termFrequency[word]\n        \n#         #calculateInverseDocumentFrequency\n#         if word in inverseDocumentFrequency and termFrequency[word] == (1/numberOfWords): # if the word exist in previous documents but not in this document yet then update it \n#             documentFrequency[word] += 1\n#             inverseDocumentFrequency[word] = np.log(numberOfDocument/documentFrequency[word])\n\n#         elif word not in inverseDocumentFrequency:\n#             documentFrequency[word] = 1\n#             inverseDocumentFrequency[word] = np.log(numberOfDocument/1)\n        \n#         tf_idf[documentIndex, word] = termFrequency[word] * inverseDocumentFrequency[word]\n        \n\n#     return termFrequency\n\n\n\n\n# #For the IDF part, each review is considered as a document. \n# #Apply function on review column\n# def tfIDF():\n#     inverseDocumentFrequency = {}\n#     numberOfDocument = len(df['review'])\n#     documentFrequency = {}\n#     nltk.download('stopwords')\n#     stopwords = nltk.corpus.stopwords.words('english') #STOP WORDS\n#     tf_idf = {}\n\n#     for index in df.index:\n    \n#         reviews = df['review'][index].lower()\n#         wordList = reviews.split(' ')\n#         termFrequencyForEachDocument = calculateTermFrequency(wordList,numberOfDocument,index)\n#     return tf_idf\n\n# tf_idf = tfIDF()\n","block_group":"feb7b1caac9549489d569457f77a24cb","execution_count":0,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"3f1bc226fbeb45489863997ec1cb5948","source_hash":"a331539","execution_start":1678720175170,"execution_millis":209751359,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"source":"# del(stopwords)\n# del(inverseDocumentFrequency)\n# del(documentFrequency)\n\n# gc.collect()","block_group":"3f1bc226fbeb45489863997ec1cb5948","execution_count":0,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"0163233dd3544ad297e7c509a5631947","source_hash":"fe2daf0a","execution_start":1678716611017,"execution_millis":213315514,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"source":"\n# sorted_tf_idf = sorted(tf_idf.items(), key=lambda x:x[1])\n# len(sorted_tf_idf)","block_group":"0163233dd3544ad297e7c509a5631947","execution_count":0,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"d39d27b308884f61bed182e238afc380","source_hash":"39144916","execution_start":1678721353073,"execution_millis":208573537,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"source":"# def calculateTopThree():\n#     topThreeList = pd.DataFrame(index=range(numberOfDocument))\n#     for index in topThreeList.index:\n#         first = {}\n#         second = {}\n#         third = {}\n\n#         oneDocTfIdf = {}\n#         for key, value in tf_idf.items():\n#             if index == key[0]:\n#                 oneDocTfIdf[key[0],key[1]] = (value) \n        \n#         sorted_oneDocTfIdf = sorted(oneDocTfIdf.items(), key=lambda x:x[1])\n#         first=sorted_oneDocTfIdf[0]\n#         second=sorted_oneDocTfIdf[0]\n#         third=sorted_oneDocTfIdf[0]\n\n#This part took 35 minutes and gave this error\n#KernelInterrupted: Execution interrupted by the Jupyter kernel.\n","block_group":"d39d27b308884f61bed182e238afc380","execution_count":0,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"4a1c6452e42749e9904e3d6c4c5a0f3f","source_hash":"1137c5df","execution_start":1678720198752,"execution_millis":209727858,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"source":"# print(sorted_tf_idf[0]) #in row 31240, the word movie has lots of repetition. \n# print(sorted_tf_idf[1])\n# print(sorted_tf_idf[2])\n# print(sorted_tf_idf[4974040])\n\n# print(np.log(numberOfDocument/1))\n","block_group":"4a1c6452e42749e9904e3d6c4c5a0f3f","execution_count":0,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"c1344ba023d74fcbaeacdef943040be9","source_hash":"6d511915","deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"source":"# highestTopThreeList = pd.DataFrame()\n# for i,rows in enumarate(tf_idf):\n#     if int(rows.keys()[0]) == i and ","block_group":"c1344ba023d74fcbaeacdef943040be9","execution_count":0,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"2cc039b706404439a673485e05cf1415","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"OK so here is how we use the tf idf i think:","block_group":"2cc039b706404439a673485e05cf1415"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"68bf893b-27a0-43a9-9b55-69d106a22c06","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"imagine we have a sentence \"I love this movie.\" and we sort the highest tf-idf thing.  \"love\" 0.9, \"I\" 0.8 and form a sentence","block_group":"68bf893b-27a0-43a9-9b55-69d106a22c06"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"d9cee953-f02f-40b8-ad14-68d8d9e26c60","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"And we select maybe 80% sentence as the trainning set and we use them to vote, for example, if 5000 cases in the trainning set with \"love\" says it is positive and 3000 cases with \"love\" says it is negative, after the vote we predict this is postive.","block_group":"d9cee953-f02f-40b8-ad14-68d8d9e26c60"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"239149a5-ffc9-45a7-bb07-137c38d02cac","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"And each vote has certain weights. which means \"love\" has 0.9 votes and \"I\" has 0.8 votes.","block_group":"239149a5-ffc9-45a7-bb07-137c38d02cac"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"8b28d0ece70f4b0898d7024556581672","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"negative or positive (prediction) I think i understand your point. Lets keep the notes!","block_group":"8b28d0ece70f4b0898d7024556581672"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"55520f64-6682-4983-a2cd-3fe4ab98d4cd","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"For review in X_test:\n          For word in review:\n                  Dictionary(tf*idf) \n          Sort and store highest\n          For word in dictionary:\n                  For train_review in X_train:\n                         Vote","block_group":"55520f64-6682-4983-a2cd-3fe4ab98d4cd"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"bad58393-c281-4b95-aade-4f5df352c49c","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"","block_group":"bad58393-c281-4b95-aade-4f5df352c49c"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"d5ce792a-25b9-48b9-b370-dec9cec336fd","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"TODO => Stemming","block_group":"d5ce792a-25b9-48b9-b370-dec9cec336fd"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"9db19690-7873-4b49-b488-59c0480d5ce1","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"TODO =>Lemmatisation","block_group":"9db19690-7873-4b49-b488-59c0480d5ce1"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"f8d57966-5caa-4702-9982-be5ef6074647","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"},"source":"TODO => Number Conversion","block_group":"f8d57966-5caa-4702-9982-be5ef6074647"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=1830d32f-c8ce-4fc8-8446-4b86a746869d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"c049843ce8044a88ba022c4958a4d5be","deepnote_execution_queue":[]}}